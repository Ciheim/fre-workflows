#!/bin/bash
set -euo pipefail
set -x

#
# Create per-variable timeseries from shards
#

export input_file_list=
export output_file_list=
echo "[COLE] created i/o lists"

# Create timeseries for a given source directory location
function process_timeseries {
    # Loop over the leaf directories which contain the datetime info
    dirs=$(find . -mindepth 2 -type d)
    if [[ -z $dirs ]]; then
        echo ERROR: No input directories found!
        exit 1
    fi

    for dir in $dirs; do
        freq=$(echo $dir | cut -d/ -f2)
        chunk=$(echo $dir | cut -d/ -f3)

        # Need a better way to compare durations, i.e. P1Y equals P12M
        if [[ $inputChunk == P12M ]]; then
            inputChunk=P1Y
        fi

        if [[ $chunk != $inputChunk ]]; then
            continue
        fi

        pushd $dir

        # Loop over variables within suitable input directories
        for var in $(ls | xargs -n1 | cut -d. -f3 | sort | uniq); do
            echo Evaluating variable $var

            # tiled or non-tiled
            if ls $component.*.$var.tile1.nc; then
                is_tiled=1
            else
                is_tiled=""
            fi

            # for tiled files, use tile1 for the time range comparisons
            files=()
            for file in $(ls *.$var?(.tile1).nc); do
                date1=$(parse_date $(echo $file | cut -d. -f2 | cut -d- -f1))
                if [[ $date1 > $begin || $date1 == $begin ]] && [[ $date1 < $end ]]; then
                    files+=($file)
                fi
            done

            if (( ${#files[@]} != $expectedChunks )); then
                # Not sure what to do here- this is unexpected to us but possibly expected by user
                echo "WARNING: Skipping $var as unexpected number of chunks; expected '$expectedChunks', got '${#files[@]}'"
                continue
            fi

            # form new filename
            d1=$(echo ${files[0]} | cut -d. -f 2 | cut -d- -f 1)
            d2=$(echo ${files[-1]} | cut -d. -f 2 | cut -d- -f 2)

            # If processing subdirectories, add the subdir (e.g. "1deg") above the component level
            if [[ $use_subdirs ]]; then
                inputdir=$subdir/$component/$freq/$chunk
                outputdir=$subdir/$component/$freq/$outputChunk
                newdir=$outputDir/$subdir/$component/$freq/$outputChunk
            else
                inputdir=$component/$freq/$chunk
                outputdir=$component/$freq/$outputChunk
                newdir=$outputDir/$component/$freq/$outputChunk
            fi

            # create timeseries
            mkdir -p $newdir

            if [[ $is_tiled ]]; then
                for (( tile=1; tile <= 6; ++tile )); do
                    newfile=$component.$d1-$d2.$var.tile$tile.nc
                    tiled_files=$(echo ${files[@]} | sed -e s/tile1/tile$tile/g)
                    for input_file in $tiled_files; do
                        hash_val=$(/home/Cole.Harvey/.conda/envs/bloom-filter-env/bin/python /home/Cole.Harvey/postprocessing/data_lineage/bloomfilter/HashGen.py $input_file)
                        export input_file_list="${input_file_list}$inputdir/$input_file  $hash_val,"
                        echo -e "\n[COLE] added $inputdir/$input_file to input list with hash_val: $hash_val"
                    done
                    cdo --history -O mergetime $tiled_files $newdir/$newfile
                done
            else
                for input_file in ${files[@]}; do
                    hash_val=$(/home/Cole.Harvey/.conda/envs/bloom-filter-env/bin/python /home/Cole.Harvey/postprocessing/data_lineage/bloomfilter/HashGen.py $input_file)
                    export input_file_list="${input_file_list}$inputdir/$input_file  $hash_val,"
                    echo -e "\n[COLE] added $inputdir/$input_file to input list with hash_val: $hash_val"
                done
                newfile=$component.$d1-$d2.$var.nc
                cdo --history -O mergetime ${files[@]} $newdir/$newfile
            fi

            current_loc=$(pwd)

            cd $newdir
                hash_val=$(/home/Cole.Harvey/.conda/envs/bloom-filter-env/bin/python /home/Cole.Harvey/postprocessing/data_lineage/bloomfilter/HashGen.py $newfile)
                export output_file_list="${output_file_list}$outputdir/$newfile  $hash_val,"
                echo -e "\n[COLE] added $outputdir/$newfile to output list with hash_val: $hash_val"
            cd $current_loc

            did_something=1
        done
        popd
    done
}

source $(dirname ${BASH_SOURCE[0]})/../shared/shared.sh

shopt -s extglob

echo Arguments:
echo "    input dir: $inputDir"
echo "    output dir: $outputDir"
echo "    begin: $begin"
echo "    input chunk: $inputChunk"
echo "    output chunk: $outputChunk"
echo "    component: $component"
echo "    components allowed to fail: ${fail_ok_components:=}"
echo "    use subdirs: ${use_subdirs:=}"
echo Utilities:
type cdo
type isodatetime

# Determine how many expected files to concatenate
expectedChunks=$(( $(isodatetime --as-total=H $outputChunk | sed -e 's/\.0$//') / $(isodatetime --as-total=H $inputChunk | sed -e 's/\.0$//') ))
if (( expectedChunks > 0 )); then
    echo NOTE: Expect to concatenate $expectedChunks subchunks
else
    echo "ERROR: Could not calculate number of expected chunks from input chunk '$inputChunk' and output chunk '$outputChunk'"
fi

# Verify input directory exists and is a directory
if [[ ! -d $inputDir ]]; then
    echo "Error: Input directory '${inputDir}' does not exist or isn't a directory"
    exit 1
fi

# Verify output directory exists and is a directory
if [[ ! -d $outputDir ]]; then
    echo "Error: Output directory '${outputDir}' does not exist or isn't a directory"
    exit 1
fi

# Calculate end date
end=$(isodatetime $begin --offset $outputChunk)
# remove trailing Z to allow string comparison later
begin=${begin%Z}
end=${end%Z}
echo "NOTE: Expect output date range to be [$begin, $end)"

cd $inputDir

# yuck
did_something=""

epmt annotate EPMT_DATA_LINEAGE_IN_PATH="$inputDir/"
echo -e "\n[COLE] annotated $inputDir to EPMT_DATA_LINEAGE_IN_PATH"

epmt annotate EPMT_DATA_LINEAGE_OUT_PATH="$outputDir/"
echo -e  "\n[COLE] annotated $outputDir to EPMT_DATA_LINEAGE_OUT_PATH"

# If processing grid subdirectories, process each
if [[ $use_subdirs ]]; then
    for subdir in $(ls); do
        pushd $subdir/$component || continue
        process_timeseries
        popd
    done
# Otherwise, process the one location
else
    cd $component
    process_timeseries
fi

echo "[COLE] epmt analysis and annotate"
which epmt

if [[ -n "$input_file_list" ]]; then
    echo -e "\n---input---"
    echo -e "epmt annotate DATA_LINEAGE_IN = \n	${input_file_list%*,}"
    epmt annotate EPMT_DATA_LINEAGE_IN="${input_file_list%*,}"
fi

if [ -n "$output_file_list" ]; then
    echo -e "\n---output---"
    echo -e "epmt annotate DATA_LINEAGE_OUT = \n    ${output_file_list%*,}"
    epmt annotate EPMT_DATA_LINEAGE_OUT="${output_file_list%*,}"
fi

if [[ -n $did_something ]]; then
    echo Natural end of the timeseries generation
    exit 0
else
    if echo $fail_ok_components | grep "\b$component\b"; then
        echo No input files were found, but this is allowed
        exit 0
    else
        echo No input files were found
        exit 1
    fi
fi
